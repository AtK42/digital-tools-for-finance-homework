{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of prominent Macroeconomic factors on the Housing Price Index in the United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "- Purchase only house price index: asdf\n",
    "- Unemployment rate: asdf\n",
    "- Consumer price index: asdf\n",
    "- Inflation rate US: asdf\n",
    "- Consumer sentiment: asdf\n",
    "- Producer price index construction machinery manufacturing: asdf\n",
    "- Average mortgage interest: asdf\n",
    "- Personal income: asdf\n",
    "- Moody's seasoned AAA corporate bond yield: asdf\n",
    "- Home supply: asdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define series_id\n",
    "series_id_dict = {\"house_pi\": 'CSUSHPISA',\n",
    "                 \"unemp_rate\": \"UNRATE\",\n",
    "                 \"CPI\": \"USACPIALLMINMEI\",\n",
    "                 \"cons_sent\": \"UMCSENT\",\n",
    "                 \"PI_const\": \"PCU333120333120\",\n",
    "                 \"mortgage_int\": \"MORTGAGE30US\",\n",
    "                 \"personal_inc\": \"PI\",\n",
    "                 \"corp_yield\": \"AAA\",\n",
    "                 \"home_supply\": \"MSACSR\",\n",
    "                 \"population\": 'POPTHM',\n",
    "                 \"new_houses\": 'HOUST',\n",
    "                 \"working_population\": 'LFWA64TTUSM647S',\n",
    "                 \"NASDAQ\": 'NASDAQCOM',\n",
    "                 \"VIX\": 'VIXCLS'}\n",
    "\n",
    "# get data folder path\n",
    "project_name = \"housing-price-index-us\"\n",
    "raw_data_path = os.path.join(os.environ[\"RESEARCH_PATH\"], project_name, \"data\", \"raw\")\n",
    "\n",
    "# get filename\n",
    "observation_start = \"2000-01-01\"\n",
    "observation_end = \"2022-06-30\"\n",
    "\n",
    "filename_list = []\n",
    "for id in series_id_dict:\n",
    "    filename_list.append(f\"data_{series_id_dict[id]}_{observation_start}_to_{observation_end}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first column (date) to start the df\n",
    "data = pd.read_feather(os.path.join(raw_data_path, filename_list[0])).iloc[:,0:1]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data together in a single pd.DataFrame\n",
    "temp = 0\n",
    "for key, value in series_id_dict.items():\n",
    "    data[key] = pd.read_feather(os.path.join(raw_data_path, filename_list[temp])).iloc[:,1:]\n",
    "    temp += 1\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type to float for all columns (but date)\n",
    "for key, value in series_id_dict.items():\n",
    "    data[key] = pd.to_numeric(data[key])\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preliminary/high-level EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.head())\n",
    "print('******************************************************************************************')\n",
    "#print(data.shape)\n",
    "print('******************************************************************************************')\n",
    "#print(data.dtypes) # all floats except for 'date' which is datetime\n",
    "print('******************************************************************************************')\n",
    "#print(data.info())\n",
    "print('******************************************************************************************')\n",
    "print(data.isnull().sum()) # no null data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some further EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how each variable evolves over time\n",
    "cols = data.columns[1:,] # don't plot date against itself\n",
    "n_rows = math.ceil(len(cols) / 4)\n",
    "figsize_height = n_rows * 5\n",
    "k = 1\n",
    "plt.figure(figsize = (25, figsize_height))\n",
    "for i in cols:\n",
    "    plt.subplot(n_rows, 4, k)\n",
    "    sns.scatterplot(data = data, x = data['date'], y = i, size = 5, legend = False)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "- 'house_pi' has a clearly upwards-sloping trend with a serious uptick in the years leading up to the year 2007/08. This is not surprising as we know today that in the build-up of the subprime crisis real estate prices skyrocketed before the housing price bubble burst. After a short period of price corrections up until ca. the year 2012, the prices have again risen and still are.\n",
    "- 'unemp_rate' has seen upticks after every major crisis - dotcom bubble in the early 21st century, the financial crisis starting in 2008 and very recently the Corona pandemic.\n",
    "- 'CPI' has steadily risen since the the beginning of the 21st century. Since and because of the corona pandemic and additionally fuelled by the on-going war in eastern Europe, we have seen major yoy increases in inflation. Central banks around the world have tried to get inflation under control by increasing interest rates which has not shown much effect up to the end of our sample time.\n",
    "- 'cons_sent' paints a somewhat cyclical picture, marked by the recessions caused by the big crises in the 21st century thus far, as mentioned above.\n",
    "- 'PI_const' looks, unsurprisingly, very similar to the broader 'CSI'. As such, it is also in line with our expectations.\n",
    "- Up until ca. 2020, 'mortagage_int' exhibits a clear downwards-sloping trend sparked yet again by the recent crises. Since 2020, however, we observe a turnaround which is in close connection to the effort of central banks (especially the FED) to curb inflation.\n",
    "- 'personal_inc' has experienced a steady upwards-sloping trend since the start date of our data set which obviously follows from the increase in inflation over the past centuries. Note, however, that naturally due to the nature of salary negotiations, the line is much smoother compared to the one of 'CPI'.\n",
    "- 'corp_yield' has a very similar evolution over time as 'mortgage_int' which is not particularly surprising since for both, the prime rate has a very big impact.\n",
    "- The 'home_supply' has a slight upwards-sloping trend with a huge upwards move shortly before the subprime crisis. Notice, that while 'house_pi' started increasing rapidly after its low point, 'home_supply' had more of a slow trod before its volatilty started increasing from the year 2020 onwards.\n",
    "- 'population' is strictly monotonically increasing which makes sense.\n",
    "- 'new_houses' had mainly increasing values except for the subprime crisis. After that, the number of new houses for sale have again steadily increased at seemingly the same rate as before the crisis.\n",
    "- 'working_population' behaves very much like 'population' as expected. One difference is, however, that 'population' is smoother which also makes sense since growth in the population is naturally much more steady."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all variables against the housing price index\n",
    "cols = data.columns[2:,] # don't plot housing index against itself + see above for plot against time\n",
    "n_rows = math.ceil(len(cols) / 4)\n",
    "figsize_height = n_rows * 5\n",
    "k = 1\n",
    "plt.figure(figsize = (25, figsize_height))\n",
    "for i in cols:\n",
    "    plt.subplot(n_rows, 4, k)\n",
    "    sns.scatterplot(data = data, x = i, y = data['house_pi'], size = 5, legend = False)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for differences\n",
    "## plot\n",
    "if data.columns[0] == 'date':\n",
    "    data_date = data.iloc[:,0]\n",
    "    data_date = data_date.iloc[1:]\n",
    "\n",
    "data_diff = data.iloc[:,1:].diff()\n",
    "data_diff = data_diff.iloc[1:,:]\n",
    "\n",
    "# plot how each variable evolves over time\n",
    "cols = data_diff.columns\n",
    "n_rows = math.ceil(len(cols) / 4)\n",
    "figsize_height = n_rows * 5\n",
    "k = 1\n",
    "plt.figure(figsize = (25, figsize_height))\n",
    "for i in cols:\n",
    "    plt.subplot(n_rows, 4, k)\n",
    "    sns.scatterplot(data = data_diff, x = data_date, y = i, size = 5, legend = False)\n",
    "    k += 1\n",
    "\n",
    "# plot all variables against the housing price index\n",
    "cols = data_diff.columns[1:] # don't plot housing index against itself\n",
    "n_rows = math.ceil(len(cols) / 4)\n",
    "figsize_height = n_rows * 5\n",
    "k = 1\n",
    "plt.figure(figsize = (25, figsize_height))\n",
    "for i in cols:\n",
    "    plt.subplot(n_rows, 4, k)\n",
    "    sns.scatterplot(data = data_diff, x = i, y = data_diff['house_pi'], size = 5, legend = False)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the variables\n",
    "cols = data.columns[1:,] #don't plot time since it's obviously uniformely distributed\n",
    "n_rows = math.ceil(len(cols) / 4)\n",
    "figsize_height = n_rows * 5\n",
    "k = 1\n",
    "plt.figure(figsize = (25, figsize_height))\n",
    "for i in cols:\n",
    "    plt.subplot(n_rows, 4, k)\n",
    "    sns.histplot(data = data[i], stat = 'probability', kde = True)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between the variables\n",
    "plt.figure(figsize = (12, 5))\n",
    "sns.heatmap(data.corr(), cmap = 'RdBu', annot = True)\n",
    "plt.title('Correlation between variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the correlation between the independent variables is high, we have an indication that we have multicollinearity which will cause problems later in the estimation and interpretation.\n",
    "\n",
    "We see that the following independent variables have high correlation:\n",
    "- unemployment rate vs. consumer sentiment: makes sense since during a recessions and/or times of crisis we have high unemployment rates as well as a diminuished consumer sentiment. Hence, also the negative correlation makes sense here.\n",
    "- CPI vs. price index construction, mortagage interest rate, personal income and corporate yield:\n",
    "    - CPI vs. price index construction: makes sense since if the price of goods in general increase, then it also increases for the construction sector.\n",
    "    - CPI vs. mortagage interest rate and corporate yield: makes sense since one action central banks take when inflation increases, is to increase interest rates (see current world situation regarding interest rate hikes).\n",
    "    - CPI vs. personal income: makes sense to some extent since if the price of goods increase, then employees ask for higher salaries to compensate for the inflation. This, however, usually happens with some delay since salaries are usually very \"sticky\".\n",
    "- price index construction vs. mortgage interest rate, personal income and corporate yield: makes sense, see explanations for CPI vs. other variables above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for differences\n",
    "# correlation between the variables\n",
    "plt.figure(figsize = (12, 5))\n",
    "sns.heatmap(data_diff.corr(), cmap = 'RdBu', annot = True)\n",
    "plt.title('Correlation between variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add year and month (insert only here, i.e., after taking differences since otherwise we'd only have 1 for year or month change)\n",
    "data_date_year = pd.to_datetime(data_date).dt.year\n",
    "data_date_month = pd.to_datetime(data_date).dt.month\n",
    "\n",
    "data_diff.insert(0, 'year', data_date_year)\n",
    "data_diff.insert(0, 'month', data_date_month)\n",
    "\n",
    "# initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# apply scaler\n",
    "data_diff_scaled = pd.DataFrame(scaler.fit_transform(data_diff), columns = data_diff.columns)\n",
    "\n",
    "# have a look at the data\n",
    "#data_diff_scaled.head()\n",
    "\n",
    "# make train-test split\n",
    "data_diff_scaled_train, data_diff_scaled_test = train_test_split(data_diff_scaled, train_size = 0.7, random_state = 100, shuffle = False)\n",
    "\n",
    "# have a look at the data\n",
    "#data_diff_scaled_train.head()\n",
    "\n",
    "# divide data into dependent and independent variable(s)\n",
    "dep_train = data_diff_scaled_train.pop('house_pi')\n",
    "indep_train = data_diff_scaled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: all predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with all predictors WITHOUT constant\n",
    "lm1_1 = sm.OLS(dep_train, indep_train).fit()\n",
    "\n",
    "# print  summary\n",
    "print(lm1_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with all predictors WITH constant\n",
    "indep_train1_2 = sm.add_constant(indep_train)\n",
    "lm1_2 = sm.OLS(dep_train, indep_train1_2).fit()\n",
    "\n",
    "# print  summary\n",
    "print(lm1_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: using only variables chosen by RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RFE and LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Running RFE\n",
    "lm_rfe = LinearRegression()\n",
    "lm_rfe.fit(indep_train, dep_train)\n",
    "\n",
    "rfe = RFE(lm_rfe)\n",
    "rfe = rfe.fit(indep_train, dep_train)\n",
    "\n",
    "#List of variables selected\n",
    "list(zip(indep_train.columns, rfe.support_, rfe.ranking_))\n",
    "\n",
    "#Columns where RFE support is True\n",
    "indep_RFE_train = indep_train.columns[rfe.support_]\n",
    "indep_RFE_train = list(indep_RFE_train)\n",
    "indep_RFE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using only variables chosen by RFE WITHOUT constant\n",
    "indep_train2_1 = indep_train[indep_RFE_train].copy()\n",
    "\n",
    "# fit another model taking out consumer sentiment, personal income and home supply\n",
    "lm2_1 = sm.OLS(dep_train, indep_train2_1).fit()\n",
    "\n",
    "# print  summary\n",
    "print(lm2_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using only variables chosen by RFE but WITH constant\n",
    "indep_train2_2 = indep_train[indep_RFE_train].copy()\n",
    "indep_train2_2 = sm.add_constant(indep_train2_2)\n",
    "\n",
    "# fit another model taking out consumer sentiment, personal income and home supply\n",
    "lm2_2 = sm.OLS(dep_train, indep_train2_2).fit()\n",
    "\n",
    "# print  summary\n",
    "print(lm2_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: using model 2 but dropping insignificant predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using only variables chosen by RFE and dropping insignificant predictors from model 2 WITHOUT constant\n",
    "indep_train3_1 = indep_train2_1[['year', 'unemp_rate', 'population', 'working_population']].copy()\n",
    "\n",
    "# fit another model taking out consumer sentiment, personal income and home supply\n",
    "lm3_1 = sm.OLS(dep_train, indep_train3_1).fit()\n",
    "\n",
    "# print  summary\n",
    "print(lm3_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using only variables chosen by RFE and dropping insignificant predictors from model 2 WITHOUT constant\n",
    "indep_train3_2 = indep_train2_1[['year', 'unemp_rate', 'population', 'working_population']].copy()\n",
    "indep_train3_2 = sm.add_constant(indep_train3_2)\n",
    "\n",
    "# fit another model taking out consumer sentiment, personal income and home supply\n",
    "lm3_2 = sm.OLS(dep_train, indep_train3_2).fit()\n",
    "\n",
    "# print  summary\n",
    "print(lm3_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('***************************************************************************************')\n",
    "#print('all variables, without constant')\n",
    "#print(round(lm1_1.aic, 3), '   ', round(lm1_1.bic, 2))\n",
    "#print('all variables, with constant')\n",
    "#print(round(lm1_2.aic, 3), '   ', round(lm1_2.bic, 2))\n",
    "#print('***************************************************************************************')\n",
    "#print('only variables chosen by RFE, without constant')\n",
    "#print(round(lm2_1.aic, 3), '   ', round(lm2_1.bic, 2))\n",
    "#print('only variables chosen by RFE, with constant')\n",
    "#print(round(lm2_2.aic, 3), '   ', round(lm2_2.bic, 2))\n",
    "#print('***************************************************************************************')\n",
    "#print('only variables chosen by RFE but without insignificant predictors, without constant')\n",
    "#print(round(lm3_1.aic, 3), '   ', round(lm3_1.bic, 2))\n",
    "#print('only variables chosen by RFE but without insignificant predictors, with constant')\n",
    "#print(round(lm3_2.aic, 3), '   ', round(lm3_2.bic, 2))\n",
    "#print('***************************************************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IC =    {'model':   ['all variables, without constant',\n",
    "                     'all variables, with constant',\n",
    "                     'RFE variables, without constant',\n",
    "                     'RFE variables, with constant',\n",
    "                     'RFE variables without insignificant predictors, without constant',\n",
    "                     'RFE variables without insignificant predictors, with constant'],\n",
    "         'AIC':     [round(lm1_1.aic, 3),\n",
    "                     round(lm1_2.aic, 3),\n",
    "                     round(lm2_1.aic, 3),\n",
    "                     round(lm2_2.aic, 3),\n",
    "                     round(lm3_1.aic, 3),\n",
    "                     round(lm3_2.aic, 3)],\n",
    "         'BIC':     [round(lm1_1.bic, 3),\n",
    "                     round(lm1_2.bic, 3),\n",
    "                     round(lm2_1.bic, 3),\n",
    "                     round(lm2_2.bic, 3),\n",
    "                     round(lm3_1.bic, 3),\n",
    "                     round(lm3_2.bic, 3)]}\n",
    "IC_df = pd.DataFrame(data = IC)\n",
    "IC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate VIF for the best model according to the AIC\n",
    "vif = pd.DataFrame()\n",
    "X = indep_train2_2\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "print(vif)\n",
    "print(\"***************************\")\n",
    "# calculate VIF for the best model according to the BIC\n",
    "vif = pd.DataFrame()\n",
    "X = indep_train3_2\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging the results from the VIF, we get satisfying results (we use as rule of thumb values below four are fine) since the value for the constant can be disregarded. We thus have no problems regarding multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model (OLS) assumptions\n",
    "0. linearity\n",
    "1. fitted values vs. residuals\n",
    "2. Normal Q-Q plot\n",
    "3. fitted values vs. sqrt of standardized residuals\n",
    "4. Cook's distance (influential points & outliers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f0a568aece6fe8ed664c83975fa83cefa82d68b5487051d6ce307809a182275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
